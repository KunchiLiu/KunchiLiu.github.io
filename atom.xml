<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>北流的笔记小屋</title>
  <icon>https://www.gravatar.com/avatar/b6c82ca6853b86194fc0dc8fd896d496</icon>
  <subtitle>近城远山，都是人间。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-10-15T07:01:59.951Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>刘坤池</name>
    <email>liukunchi19@otcaix.iscas.ac.cn</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>社区发现方法调研-02</title>
    <link href="http://yoursite.com/2020/10/15/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E6%96%B9%E6%B3%95%E8%B0%83%E7%A0%94-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2020/10/15/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E6%96%B9%E6%B3%95%E8%B0%83%E7%A0%94-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-10-15T05:58:53.000Z</published>
    <updated>2020-10-15T07:01:59.951Z</updated>
    
    <content type="html"><![CDATA[<p>这次调研主要是从参考了发表在IJCAI 2020上一篇综述《Deep Learning for Community Detection: Progress, Challenges and Opportunities》，作者以三种广泛的深度学习方法回顾了社区检测领域中模型和算法开发的技术趋势以及当前的发展状况，并确定了需要通过深度学习来克服社区发现的七个挑战。</p><a id="more"></a><ul><li><p>Deep Learning for Community Detection: Progress, Challenges and Opportunities</p></li><li><p>IJCAI2020</p></li><li><p>该综述调研的文章主要来自：NIPS, ICLR, AAAI, IJCAI, KDD, ICDM, CIKM, ICDE和WWW，也包括一些高质量的同行评审期刊(peer-reviewed journals)。</p><p><img src="/2020/10/15/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E6%96%B9%E6%B3%95%E8%B0%83%E7%A0%94-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/title.png" alt="文章标题"></p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>网络/图的两个基本元素是节点和边。从连通性和密度的角度来看，社区被称为局部密集的连通子图或节点簇（clusters of nodes）。除了子图的内部衔接之外，还应考虑它们之间的分隔。为此，图论提出了两个特定的规则来划分社区：</p><ul><li>社区中的节点紧密连接；</li><li>不同社区中的节点稀疏连接。</li></ul><p><img src="/2020/10/15/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E6%96%B9%E6%B3%95%E8%B0%83%E7%A0%94-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/An example of community detection in a social network.png" alt="社交网络的社区划分"></p><p>网络中同一个社区内部的节点具有相似的观点、功能(function)或者意识(purpose)，可以帮助我们了解网络固有的模式和功能。例如，蛋白质互作用（PPI）网络中的社区检测用于发现具有相似生物学功能的蛋白质；在引文网络（citation networks）中，社区检测确定了各个研究主题的重要性、相互联系及其发展；在企业网络中，可以通过研究公司员工的线上社交关系以及线下内部资源管理对其进行分组；在Twitter和Facebook等社交网络中，具有共同兴趣或共同朋友的用户可能是同一社区的成员。</p><p>大多数传统的社区检测方法都基于统计推断和常见的机器学习的方法等。在传统的机器学习中，社区检测通常被认为是图上的聚类问题。但是这些方法高度依赖于数据的特征，例如，使用节点特征向量将节点划分为社区的谱聚类（2002）。随机块模型（2011）是检测社区并描述社区形成方式的最广泛使用的方法之一，但它不能解决当今复杂的数据集和复杂的社交场景下所带来的一些列问题。因此，网络规模和数据维度的扩大需要更强大的技术来以可行的计算速度保证其高效的性能。</p><ul><li>2011 Phys. Rev. E - Stochastic blockmodels and community structure in networks.</li></ul><p>至少就目前而言，深度学习是一种解决方案。通过深度学习，计算模型可以在多个抽象级别上学习数据的表示形式，这非常适合于网络数据。不仅其学习非线性特征的能力得到了极大的提高，而且可以降低数据的维度，从而扩大了网络分析任务的范围（如社区检测，节点分类和链接预测等）。</p><p>这篇文章从模型的角度对社区检测方法进行分类，可分为：基于深度神经网络（Deep Neural Networks）的方法，基于深度图嵌入（Deep Graph Embedding）的方法和基于图神经网络（Graph Neural Networks）的方法，最后介绍了当前尚未解决的挑战和未来研究方向。</p><h2 id="相关定义"><a href="#相关定义" class="headerlink" title="相关定义"></a>相关定义</h2><p><strong>定义1（网络）</strong>根据图论，加权网络表示为<script type="math/tex">G=(V,E,W)</script>，未加权网络表示为$G=(V,E)$，其中$V$和$E$表示节点和边的集合，$W$分别表示$E$相应的权重，以连接的强度或容量为单位。在未加权的网络中，$W$被视为1。子图$g \subseteq G$是保留原始网络结构的图划分。子图的划分遵循预定义（pre-define）的规则，不同的规则可能会导致不同形式的子图。社区是代表真实社会现象的一种子图。换句话说，社区是一组具有共同特征的人或对象。</p><p><strong>定义2（社区）</strong>社区是网络中节点密集连接的子图，稀疏连接的节点沟通了不同的社区。在这里，使用$C=\lbrace C_1,C_2,\cdots,C_k\rbrace$表示将网络$G$划分为$k$个社区的集合，其中$C_i$是社区划分的第$i$个社区。节点$v$属于社区$C_i$满足如下条件：社区内部每个节点的内部度大于其外部度。</p><p>因此，社区检测的目标是发现网络$G$中的社区$C$。</p><h2 id="为什么要用深度学习做社区检测？"><a href="#为什么要用深度学习做社区检测？" class="headerlink" title="为什么要用深度学习做社区检测？"></a>为什么要用深度学习做社区检测？</h2><p>与其他机器学习方法相比，深度学习在社区检测方面的明显优势是它能够对高维数据的特征表示进行编码。深度学习模型还可以学习节点、邻域和子图的模式。另外，它们对于与大型图数据的稀疏性更具弹性（resilient）。在许多现实世界中，大多数节点都是未标记的，并且数据中的社区几乎没有先验知识，因此深度学习是无监督学习任务的最佳选择。</p><p>除了简单地利用网络拓扑来检测社区之外，一些策略还探索语义描述作为节点特征。传统的社区检测方法主要基于邻接矩阵和节点属性矩阵[ Yang et al., 2013; He et al., 2017 ]。</p><ul><li>2013 ICDM - Community detection in networks with node attributes.</li><li>2017 AAAI - Joint identiﬁcation of network communities and semantics via integrative modeling of network topologies and node contents</li></ul><p>但深度学习可以创建更强大的节点属性表示和社区结构表示。为此，最近的研究表明，在深度学习社区检测中有希望的新方向–例如，基于深度非负矩阵分解（NMF）的方法和根据社区属性修改了深度学习模型等。</p><ul><li>2018 CIKM - Deep autoencoderlike nonnegative matrix factorization for community detection</li><li>2019 Future Gener. Comput. Syst - A distributed overlapping community detection model for large graphs using autoencoder.</li><li>2019 NIPS - vGraph: A generative model for joint community detection and node representation learning</li></ul><p>深度学习可能给社区发现未来带来的可能影响包括以下四点：</p><ul><li><p>性能提升；</p></li><li><p>在更多更丰富的特征上进行社区检测的能力；</p></li><li>以网络拓扑和节点属性为基础进行检测的能力，从而建立了更鲁棒并且性能更好的模型；</li><li>检测大规模网络中更复杂结构的能力。</li></ul><h2 id="基于深度学习的社区检测"><a href="#基于深度学习的社区检测" class="headerlink" title="基于深度学习的社区检测"></a>基于深度学习的社区检测</h2><ul><li>左边是社区检测面临的七大挑战：社区数量未知、层次网络、网络异构性、边上符号信息、社区嵌入、动态网络和大规模网络；</li><li>右边是基于深度学习的社区检测方法分类，可分为三大类：基于DNN的方法，基于深度图嵌入（Deep Graph Embedding）的方法和基于GNN的方法。</li></ul><p><img src="/2020/10/15/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0%E6%96%B9%E6%B3%95%E8%B0%83%E7%A0%94-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/challenge and approaches.png" alt="基于深度学习的社区检测方法的挑战与分类"></p><h3 id="基于DNN的社区检测"><a href="#基于DNN的社区检测" class="headerlink" title="基于DNN的社区检测"></a>基于DNN的社区检测</h3><p>在社区检测中，基于DNN的三种常用模型是卷积神经网络（CNNs）、自编码器和生成对抗网络（GANs）。</p><h4 id="基于CNN的方法"><a href="#基于CNN的方法" class="headerlink" title="基于CNN的方法"></a>基于CNN的方法</h4><p>CNNs的两个关键组成部分是卷积操作和卷积层上的池化操作：卷积运算使用卷积内核来减少计算成本；池化操作随后应用于特征映射，以确保CNNs的鲁棒性。</p><p>利用CNNs的优势，[2017]设计了一种新颖的CNN模型，以检测拓扑不完整网络（topologically incomplete networks）中的社区，不完整是指从现实世界网络中观测时会丢失一些边。 [2019]在CNN框架中增加稀疏矩阵卷积，来特定地处理与邻接矩阵相关的高维稀疏表示。</p><ul><li>2017 Physica A - Deep community detection in topologically incomplete networks.</li><li>2019 SAC - A deep learning based community detection approach. </li></ul><h4 id="基于自编码器的方法"><a href="#基于自编码器的方法" class="headerlink" title="基于自编码器的方法"></a>基于自编码器的方法</h4><p>堆叠自编码器(Stacked auto-encoders)是用于社区检测的一种非常强大的DNN模型。将自编码器应用于社区检测的灵感来自以下发现：自编码器和谱聚类在谱矩阵的低维近似方面具有相似的框架（frameworks）[Cao et al .2018b]。对于网络拓扑方面，[Bhatia and Rani. 2018]设计一种方法通过基于随机游走的个性化PageRank来学习节点社区，并通过优化社区结构的模块度进行微调。为了利用节点属性信息，[Cao et al. 2018b]提出了一种堆叠式自编码器，该编码器通过组合网络拓扑和节点属性进行社区检测，来增强DNN中隐藏层的泛化能力；为了进一步解决网络拓扑和节点属性之间的匹配问题，[Cao et al.2018a]通过引入自适应参数作为匹配项的折衷控制(trade-off control)，提出了一种图正则化自编码器方法。</p><ul><li>2018b Neurocomputing - Incorporating network structure with node contents for community detection on large networks using deep learning</li><li>2018 Knowl. Inf. Syst - DFuzzy: A deep learning-based fuzzy clustering model for large graphs.</li><li>2018a KSEM - Autoencoder based community detection with adaptive integration of network topology and node contents.</li></ul><p>为避免需要预先设置社区的数量，分层堆叠的自编码器可以根据网络结构有效地找到社区的中心[Bhatia and Rani. 2018]。此外，这种自选择机制可确保模型严格按照社区标准划分节点。自从提出以来，越来越多的方法开始自适应地学习社区结构，而不是预先定义一个数目。例如，[Choong et al. 2018]引入了混合高斯模型，以从社区结构中捕获高阶模式（higher-order patterns），并对观察到的网络的生成过程进行建模，来检测社区。</p><ul><li>2018 Knowl. Inf. Syst - DFuzzy: A deep learning-based fuzzy clustering model for large graphs.</li><li>2018 ICDM - Learning community structure with variational autoencoder.</li></ul><p>对于具有正负符号连接的网络，它们被称为有符号网络。为了处理边上的符号信息，半监督堆叠自编码器通过重构邻接矩阵来表示有符号网络，以进一步学习网络嵌入[Shen and Chung，2018]。</p><ul><li>2018 IEEE Trans. Cybern. - Deep network embedding for graph representation learning in signed networks.</li></ul><h4 id="基于生成对抗网络（GAN）的方法"><a href="#基于生成对抗网络（GAN）的方法" class="headerlink" title="基于生成对抗网络（GAN）的方法"></a>基于生成对抗网络（GAN）的方法</h4><p>GANs涉及两个相互竞争的深度神经网络，从而可以快速调整训练精度。这些方法通常在无监督的情况下运行，从而生成具有与训练集相同统计特征的新数据，[Wang等，2019]探索了利用GAN来进行高效的图表示学习任务。</p><ul><li>2019 IEEE Trans. Knowl. Data. - Learning graph representation with generative adversarial nets. </li></ul><p>[Jia et al. 2019]认为基于图聚类的传统社区检测方法无法处理社区的密集重叠（dense overlapping），因为一个节点可能属于多个社区。为此，他们提出了一种新颖的CommunityGAN模型，该模型解决了重叠社区检测和基于GANs的图表示学习；更重要的是，与没有特定含义的图节点表示相比，CommunityGAN具有表示节点对社区的隶属强度的能力。</p><ul><li>2019 WWW - CommunityGAN: Community detection with generative adversarial nets.(*)</li></ul><h3 id="基于深度图嵌入的社区检测"><a href="#基于深度图嵌入的社区检测" class="headerlink" title="基于深度图嵌入的社区检测"></a>基于深度图嵌入的社区检测</h3><p>深度图嵌入是将网络中的节点映射到低维空间的一种技术，同时在表示中保留尽可能多的结构信息。图嵌入方法特别适合基于网络分析的机器学习任务，例如，链接预测、节点分类和节点聚类等，因为他们可以直接利用表示中的隐特征而不是去搜索网络。之后，可以利用聚类方法（例如k均值）进行社区检测。</p><h4 id="基于深度非负矩阵分解（NMF）的方法"><a href="#基于深度非负矩阵分解（NMF）的方法" class="headerlink" title="基于深度非负矩阵分解（NMF）的方法"></a>基于深度非负矩阵分解（NMF）的方法</h4><p>非负矩阵分解（NMF）可将一个矩阵分解为两个矩阵，并且这三个矩阵都没有负元素。对于社区检测，通过进一步最小化聚类任务的误差函数，将网络的邻接矩阵近似地分解为两个非负矩阵的乘积。[Ye et al, 2018]提出了一种新颖的深度NMF模型，该模型通过在深度学习过程中，将社区结构映射到原始网络结构来提升性能，并通过添加节点属性形成属性图，基于深度NMF的社区检测仅仅需要将邻接矩阵和节点属性矩阵进行分解。此外，[Li et al, 2018]根据深度特征学习和深度网络嵌入，使用NFM进行基于节点属性和社区嵌入的属性社区检测（attributed community detection）。</p><ul><li>2018 CIKM - Deep autoencoderlike nonnegative matrix factorization for community detection.(*)</li><li>2018 AAAI - Community detection in attributed graphs: an embedding approach.(*)</li></ul><h4 id="基于深度稀疏滤波（SF）的方法"><a href="#基于深度稀疏滤波（SF）的方法" class="headerlink" title="基于深度稀疏滤波（SF）的方法"></a>基于深度稀疏滤波（SF）的方法</h4><p>嵌入可以对成对关系的输入进行编码以避免搜索稀疏的邻接矩阵，因此，稀疏滤波（SF）是一种有效的深度特征学习（deep feature learning）算法，只需要一个超参数即可处理高维输入[Ngiam. 2011]。 [Xie et al. 2018]提出了一种有效的网络表示方法，该方法通过深度稀疏滤波的方式在网络社区发现中发挥作用，该方法尤其适用于大型网络。一种无监督的深度学习算法提取网络特征，然后用于社区划分。</p><ul><li>2011 NIPS - Sparse ﬁltering</li><li>2018 Pattern Recognit - Community discovery in networks with deep sparse ﬁltering</li></ul><h4 id="基于社区嵌入的方法"><a href="#基于社区嵌入的方法" class="headerlink" title="基于社区嵌入的方法"></a>基于社区嵌入的方法</h4><p>传统上，图嵌入专注于单个节点，但是网络中的社区反映了高阶相似性（high-order proximities），例如相似的观点和行为。这是图嵌入的一个重要但尚未充分研究的领域，重点是社区嵌入，其目标是了解低维空间中社区的节点分布。 [Cavallari et al，2017]调查了这一想法，认为这种新颖且简单（non-trivial）的策略可能有益于社区发现。例如，通过一种过渡（transitional）图嵌入的方法，可以使用节点分布来保留网络结构，从而反向改善社区检测。 [Zhang et al，2018]提出了一种保留社区的网络嵌入方法来学习网络表示。此外，[Tu et al，2019]提出了一种新的图嵌入模型，该模型可以学习节点和社区的嵌入，在优化过程中二者是交替进行，而非同时解决这两个任务。</p><ul><li>2017 CIKM - Learning community embedding with community detection and node embedding on graphs.(*)</li><li>2018 AAAI - Cosine: Community-preserving social network embedding from information diffusion cascades.(*)</li><li>2019 IEEE Trans. Knowl. Data Eng. - A uniﬁed framework for community detection and network representation learning.(*)</li></ul><h3 id="基于GNN的社区检测"><a href="#基于GNN的社区检测" class="headerlink" title="基于GNN的社区检测"></a>基于GNN的社区检测</h3><p>GNNs是图挖掘和深度学习的技术融合，它们的快速发展证明了它们在建模和捕获图数据中复杂关系的能力。例如，[Chen et al，2019]中用于监督社区检测的GNN引入了一个非回溯算子（a non-backtracking operator）来定义边邻接性(edge adjacency)，它不仅是提高学习性能的可行性方法，而且算子选择也很方便。</p><p>图卷积网络（GCNs）继承了CNNs快速学习的特点，并通过集成了考虑网络中实体概率分布的概率模型，进一步改善了这个优点。例如，[Jin et al. 2019]将马尔可夫随机场与语义信息属性网络相结合以支持半监督学习，而[Shchur and Gunnemann，2019]集成了伯努利-泊松（Bernoulli-Poisson）概率模型与GCN来实现重叠社区检测，使得卷积层可以识别复杂的网络模式。</p><ul><li>2019 ICLR - Supervised community detection with line graph neural networks.</li><li>2019 AAAI - Graph convolutional networks meet markov random ﬁelds: Semisupervised community detection in attribute networks.</li><li>2019 KDD Workshop DLG’19 - Overlapping community detection with graph neural networks.</li></ul><h2 id="挑战和机遇"><a href="#挑战和机遇" class="headerlink" title="挑战和机遇"></a>挑战和机遇</h2><h3 id="社区数量未知"><a href="#社区数量未知" class="headerlink" title="社区数量未知"></a>社区数量未知</h3><p>到目前为止，网络中社区数量的未知性没有得到很好的解决。在现实世界中，提取的大多数数据都没有标签，因此，在机器学习中，社区检测通常被视为无监督的聚类问题。这导致了一个左右为难（catch-22）问题：需要对数据进行标记以确定社区的数量，但是在对数据进行标记之前需要知道社区的数量。深度学习方法通过根据一个或多个隐空间中的相似性对节点进行聚类，在某种程度上解决此问题。但是，聚类算法仍然需要提前知道最终的聚类数。</p><p>机会(Opportunities:)：到目前为止，一种有效的解决方案是通过分析网络拓扑来计算社区数量，例如[Bhatia and Rani，2018; Bhatia and Rani，2019年]基于随机游走的个性化PageRank。但是，这类方法不能保证将网络中的每个节点都分配给一个社区。因此，尚未找到针对该问题完整的解决方案。</p><ul><li>2018 Knowl. Inf. Syst. - DFuzzy: A deep learning-based fuzzy clustering model for large graphs.</li><li>2019 Future Gener. Comput. Syst. - A distributed overlapping community detection model for large graphs using autoencoder.</li></ul><h3 id="层次网络（Hierarchical-Networks）"><a href="#层次网络（Hierarchical-Networks）" class="headerlink" title="层次网络（Hierarchical Networks）"></a>层次网络（Hierarchical Networks）</h3><p>层次网络由分层网络组成，其中每层网络共享特定的功能（functions）。因此，社区检测策略必须能够提取分层表示（layer-wise representations）。挑战包括：区分不同的关系类型（例如水平和垂直），以及管理（manage）不同层中不同级别（levels）的稀疏性。</p><p>机会：[Song和Thiagarajan，2018年]提出了一种多层DeepWalk，通过创建层间的边（inter-layer edges）以利用跨不同层（across different layers）的依赖性，同时保留层次结构；它学习每层中每个节点的表示，并将这些表示通过优化策略进行微调。另一个可能的解决方案是同时优化适用于所有层的公共表示和保留特定层网络结构的局部表示。此外，上述方案对于层次网络中层数的可伸缩性令人怀疑，在设计新的解决方案时应予以考虑。另外，需要新的模型来区分层次网络中不同类型的连接（links）。因此，在我们利用深度学习方法来检测层次网络的社区之前，还有大量工作要做。</p><ul><li>2019 IEEE Big Data - Improved deep embeddings for inferencing with multi-layered networks.</li></ul><h3 id="网络异构性（Network-Heterogeneity）"><a href="#网络异构性（Network-Heterogeneity）" class="headerlink" title="网络异构性（Network Heterogeneity）"></a>网络异构性（Network Heterogeneity）</h3><p>网络异质性是指包含明显不同类型的节点和边的网络，这意味着用于同构网络的策略不一定有效。特别地，在模型和算法的设计中需要解决与每种类型的节点相关的不同概率分布。</p><p>机会：迄今为止，很少有深度学习方法考虑网络异构性。[Chang et al，2015]通过非线性嵌入函数来捕获异构网络中节点之间的复杂交互，从而解决了这个问题；然而，他们的方法忽略了节点之间关系的不同语义。异构网络中社区检测的机会可能包括：</p><ol><li>深度图嵌入模型和支持算法（supporting algorithms）；</li><li>具有新颖训练过程的特定深度学习模型，以学习隐藏层中的异构图属性（heterogeneous graph properties）； </li><li>可以利用节点之间不同类型边的新模型。</li></ol><ul><li>2015 KDD - Heterogeneous network embedding via deep architectures.</li></ul><h3 id="边上符号信息（Signed-Information-on-Edges）"><a href="#边上符号信息（Signed-Information-on-Edges）" class="headerlink" title="边上符号信息（Signed Information on Edges）"></a>边上符号信息（Signed Information on Edges）</h3><p>现实世界中许多网络都有代表正负符号的边，该问题的挑战在于如何以不同的方式区别对待这样的边。</p><p>机会：一种可行的解决方案是通过设计随机行走过程来合并（incorporate）正边和负边。 [Hu et al，2019]按照这个想法，设计了一个基于词嵌入的稀疏图嵌入模型，但是对于现实世界中某些小型的带符号网络，其方法的性能不如基准(baseline)的谱方法；另一种可能解决方案是重建有符号网络的邻接矩阵表示，但是，这带来了其他问题，因为在现实世界中，邻接关系绝大多数都是正边。 [Shen and Chung，2018]通过增加了惩罚项，来确保其堆叠的自编码器模型更多地关注在大量的正边上重建稀疏的负边，但是，如果没有对社区中大多数关系的先验知识（这在许多情况下是现实的），该方法将行不通。因此，目前仍然需要在符号网络中进行无监督社区检测的有效方法。</p><ul><li>2019 J. Ambient Intell. Hum. Comput. - Sparse network embedding for community detection and sign prediction in signed social networks</li><li>2018 IEEE Trans. Cybern. - Deep network embedding for graph representation learning in signed networks.</li></ul><h3 id="社区嵌入"><a href="#社区嵌入" class="headerlink" title="社区嵌入"></a>社区嵌入</h3><p>社区嵌入的目的是创建社区的表示形式，而不是每个节点的表示形式。因此，焦点转移到了社区感知的高阶邻近度（high-order proximity），而不是与节点邻域相关的1阶或2阶邻近度。这是一个新兴的研究领域，需要克服三个主要挑战：</p><ol><li>高计算成本；</li><li>节点与社区结构之间的关系评估；</li><li>应用深度学习模型时的其他问题，例如跨社区的分布转移。</li></ol><p>机会：文中提出以下研究目标：</p><ol><li>探索如何将社区嵌入整合到深度学习模型中；</li><li>确定如何直接嵌入社区结构以获得一系列好处，例如快速计算； </li><li>在集成了深度社区检测学习的模型中，设一种计优化超参数的方法。</li></ol><h3 id="动态网络"><a href="#动态网络" class="headerlink" title="动态网络"></a>动态网络</h3><p>动态变化会影响网络拓扑或节点属性，每个属性都必须以自己的方式进行处理。拓扑更改（例如添加或删除节点或边）不仅会导致局部社区发生更改，而且还会导致整个网络的社区划分发生改变[Liu et al. 2020]。使用动态网络，需要通过一系列网络快照来重新训练深度学习模型。动态网络的时间属性（temporal attributes）所面临的技术挑战在于动态的深度特征提取。</p><ul><li>2020 WWW - Detecting the evolving community structure in dynamic social networks</li></ul><p>机会：动态网络中用于检测具有动态时空特性社区的深度学习方法还未被很好地设计出来，因此，未来的研究方向包括：</p><ol><li>检测并识别社区的空间变化（spatial changes）；</li><li>学习嵌入时间特征（temporal feature ）和社区结构信息（community structure information）的深层模式（deep patterns）；</li><li>设计一种统一深度学习方法用于社区检测，且该方法可以同时处理空间和时间特征。</li></ol><h3 id="大规模网络"><a href="#大规模网络" class="headerlink" title="大规模网络"></a>大规模网络</h3><p>当先现实世界中，大型网络可以包含数百万节点、边和结构模式（structural patterns），并且还会高度动态化，如类似Facebook和Twitter的社交网络。在大规模网络中社区发现实现之前，有许多需要解决的问题。例如，大型网络可能具有其固有的规模特征，例如社交网络中的无规模（即幂律度分布，比如完全图度分布，d=n-1的概率是1，d=0的概率是0；随机网络度分布满足正态分布；无尺度网络：大部分的节点只有比较少的连接，而少数节点有大量的连接，不存在特征度数，故称无尺度）。这种分布会影响深度学习在社区检测中的性能，可扩展性也是使深度学习能够检测大型网络中社区的另一个关键问题，而且不断变化的网络类型进一步增加了检测难度。总体而言，大规模网络中的深度社区检测涉及上述所有六个挑战以及可扩展学习的挑战。</p><p>机会：要在大规模网络中充分利用丰富的信息，就需要新的无监督聚类算法，该算法要具有较低的计算复杂度和更大的灵活性。分布式计算在大规模机器学习中很流行。因此，一个可能的方向是设计一种鲁棒的深度学习社区检测方法，且需要实现高性能的协同计算。另一方面，关于高维邻接矩阵，深度学习中常用的降维关键策略（即矩阵低秩逼近）不适用于大规模网络，即使是目前的分布式计算解决方案也是相当耗费计算资源。因此，大规模网络中社区划分中迫切需要新的深度学习框架、模型和算法。作为社区检测中最大的挑战，这些框架在准确性和速度上需要远远超过当前的基准（benchmarks）。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>传统的社区检测方法通常依赖于统计推断和常规的机器学习方法，深度学习的进步推动了社区检测策略发展，该策略具有更强大的能力来处理高维图数据。在这篇综述中，作者以三种广泛的深度学习方法回顾了社区检测领域中模型和算法开发的技术趋势以及当前的发展状况，并确定了需要通过深度学习来克服社区发现的七个挑战。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这次调研主要是从参考了发表在IJCAI 2020上一篇综述《Deep Learning for Community Detection: Progress, Challenges and Opportunities》，作者以三种广泛的深度学习方法回顾了社区检测领域中模型和算法开发的技术趋势以及当前的发展状况，并确定了需要通过深度学习来克服社区发现的七个挑战。&lt;/p&gt;
    
    </summary>
    
    
      <category term="社区发现" scheme="http://yoursite.com/categories/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="社区发现" scheme="http://yoursite.com/tags/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0/"/>
    
      <category term="社区检测" scheme="http://yoursite.com/tags/%E7%A4%BE%E5%8C%BA%E6%A3%80%E6%B5%8B/"/>
    
      <category term="图神经网络" scheme="http://yoursite.com/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="图嵌入" scheme="http://yoursite.com/tags/%E5%9B%BE%E5%B5%8C%E5%85%A5/"/>
    
      <category term="聚类" scheme="http://yoursite.com/tags/%E8%81%9A%E7%B1%BB/"/>
    
      <category term="社交网络" scheme="http://yoursite.com/tags/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Python中调用C语言代码</title>
    <link href="http://yoursite.com/2019/10/11/Python%E4%B8%AD%E8%B0%83%E7%94%A8C%E8%AF%AD%E8%A8%80%E4%BB%A3%E7%A0%81/"/>
    <id>http://yoursite.com/2019/10/11/Python%E4%B8%AD%E8%B0%83%E7%94%A8C%E8%AF%AD%E8%A8%80%E4%BB%A3%E7%A0%81/</id>
    <published>2019-10-11T10:29:06.000Z</published>
    <updated>2019-10-11T12:38:03.760Z</updated>
    
    <content type="html"><![CDATA[<p>首先，我们要明确为什么要在Python中调用C？比较常见原因如下： </p><ul><li>你要提升代码的运行速度，而且你知道C要比Python快50倍以上。</li><li>C语言中有很多传统类库，而且有些正是你想要的，但你又不想用Python去重写它们</li><li>想对从内存到文件接口这样的底层资源进行访问。 </li><li>不需要理由，Just想玩玩。</li></ul><p>更详细请点击<a href="https://blog.csdn.net/qq_35636311/article/details/78255568" target="_blank" rel="noopener">这里</a>。</p><a id="more"></a><h1 id="使用C扩展"><a href="#使用C扩展" class="headerlink" title="使用C扩展"></a>使用C扩展</h1><p>开发者有三种方法可以在自己的Python代码中来调用C编写的函数，分别是<strong>ctypes</strong>、<strong>SWIG</strong>，<strong>Python/C API</strong>，但每种方式也都有各自的利弊。</p><h1 id="Ctype"><a href="#Ctype" class="headerlink" title="Ctype"></a>Ctype</h1><p>Python中的<strong><a href="https://docs.python.org/2/library/ctypes.html" target="_blank" rel="noopener">ctypes模块</a></strong>可能是Python调用C方法中最简单的一种。ctypes模块提供了和C语言兼容的数据类型和函数来加载dll文件，因此在调用时不需对源文件做任何的修改，故该方法很简单。下面用C编写一个两数求和的<code>add.c</code>：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//sample C file to add 2 numbers - int and floats</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span>, <span class="keyword">int</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span>, <span class="keyword">float</span>)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">add_int</span><span class="params">(<span class="keyword">int</span> num1, <span class="keyword">int</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">add_float</span><span class="params">(<span class="keyword">float</span> num1, <span class="keyword">float</span> num2)</span></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> num1 + num2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来将C文件编译为<code>.so</code>文件(Windows下为DLL)。下面操作会生成<code>adder.so</code>文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#For Linux</span></span><br><span class="line">$ gcc -shared -Wl,-soname,adder -o adder.so -fPIC add.c</span><br><span class="line"> </span><br><span class="line"><span class="comment">#For Mac</span></span><br><span class="line">$ gcc -shared -Wl,-install_name,adder.so -o adder.so -fPIC add.c</span><br></pre></td></tr></table></figure><p>然后在Python中调用即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ctypes <span class="keyword">import</span> *</span><br><span class="line"> </span><br><span class="line"><span class="comment">#load the shared object file</span></span><br><span class="line">adder = CDLL(<span class="string">'./adder.so'</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Find sum of integers</span></span><br><span class="line">res_int = adder.add_int(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sum of 4 and 5 = "</span> + str(res_int))</span><br><span class="line"> </span><br><span class="line"><span class="comment">#Find sum of floats</span></span><br><span class="line">a = c_float(<span class="number">5.5</span>)</span><br><span class="line">b = c_float(<span class="number">4.1</span>)</span><br><span class="line"> </span><br><span class="line">add_float = adder.add_float</span><br><span class="line">add_float.restype = c_float</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sum of 5.5 and 4.1 = "</span>, str(add_float(a, b)))</span><br></pre></td></tr></table></figure><p>在这个例子中，C文件是自解释的，它包含两个函数，分别实现了整形求和和浮点型求和。</p><p>在Python文件中，一开始先导入ctypes模块，然后<strong>使用CDLL函数来加载我们创建的库文件</strong>。这样我们就可以通过变量<code>adder</code>来使用C类库中的函数了。当<code>adder.add_int()</code>被调用时，内部将发起一个对C函数<code>add_int()</code>的调用。ctypes接口允许我们在调用C函数时使用原生Python中默认的字符串型和整型。</p><p>而对于其他类似布尔型和浮点型这样的类型，必须要使用正确的ctype类型才可以。如向<code>adder.add_float()</code>函数传参时, 我们要先将Python中的十进制值转化为<code>c_float</code>类型，然后才能传送给C函数。这种方法虽然简单，清晰，但是却很受限。例如，并不能在C中对对象进行操作。</p><h1 id="SWIG"><a href="#SWIG" class="headerlink" title="SWIG"></a>SWIG</h1><p>SWIG是Simplified Wrapper and Interface Generator的缩写，是Python中调用C代码的另一种方法。在这个方法中，开发人员必须编写一个额外的接口文件来作为SWIG(终端工具)的入口。</p><p><strong>Python开发者一般不会采用这种方法，因为大多数情况它会带来不必要的复杂。</strong>而当你有一个C/C++代码库需要被多种语言调用时，这将是个非常不错的选择。来自<a href="http://www.swig.org/tutorial.html" target="_blank" rel="noopener">SWIG官网</a>的示例如下，但未实验：</p><p><code>example.c</code>文件中的C代码包含了不同的变量和函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="keyword">double</span> My_variable = <span class="number">3.0</span>;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">fact</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n &lt;= <span class="number">1</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> n*fact(n<span class="number">-1</span>);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">my_mod</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (x%y);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">get_time</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">time_t</span> ltime;</span><br><span class="line">    time(&amp;ltime);</span><br><span class="line">    <span class="keyword">return</span> ctime(&amp;ltime); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后编译它：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">unix % swig -python example.i</span><br><span class="line">unix % gcc -c example.c example_wrap.c -I/usr/local/include/python2.1</span><br><span class="line">unix % ld -shared example.o example_wrap.o -o _example.so</span><br></pre></td></tr></table></figure><p>最后，Python输出如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import example</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; example.fact(5)</span></span><br><span class="line">120</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; example.my_mod(7,3)</span></span><br><span class="line">1</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; example.get_time()</span></span><br><span class="line">'Sun Feb 11 23:01:07 1996'</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;</span></span><br></pre></td></tr></table></figure><p>可以看出，使用SWIG确实达到了同样的效果，虽然下了更多的工夫，但如果你的目标是多语言还是很值得的。</p><h1 id="Python-C-API"><a href="#Python-C-API" class="headerlink" title="Python/C API"></a>Python/C API</h1><p>Python/C API可能是被最广泛使用的方法。它不仅简单，而且可以在C代码中操作你的Python对象。这种方法需要以特定的方式来编写C代码以供Python去调用它。所有的Python对象都被表示为一种叫做PyObject的结构体，并且<code>Python.h</code>头文件中提供了各种操作它的函数。例如，如果PyObject表示为PyListType(列表类型)时，那么我们便可以使用<code>PyList_Size()</code>函数来获取该结构的长度，类似Python中的<code>len(list)</code>函数。大部分对Python原生对象的基础函数和操作在<code>Python.h</code>头文件中都能找到。接下来，编写一个C扩展，添加所有元素到一个Python列表(所有元素都是数字)，然后来看一下我们要实现的效果，这里示例了用Python调用C扩展的代码。</p><p>来看一下我们要实现的效果，这里演示了用Python调用C扩展的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Though it looks like an ordinary python import, the addList module is implemented in C</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l)))</span><br></pre></td></tr></table></figure><p>上面的代码和普通的Python文件并没有什么分别，导入并使用了另一个叫做<code>addList</code>的Python模块。唯一差别就是这个模块并不是用Python编写的，而是C。</p><p>接下来我们看看如何用C编写addList模块，这可能看起来有点让人难以接受，但是一旦你了解了这之中的各种组成，你就可以一往无前了。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Python.h has all the required function definitions to manipulate the Python objects</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Python.h&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//This is the function that is called from your python code</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> PyObject* <span class="title">addList_add</span><span class="params">(PyObject* self, PyObject* args)</span></span>&#123;</span><br><span class="line"> </span><br><span class="line">    PyObject * listObj;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//The input arguments come as a tuple, we parse the args to get the various variables</span></span><br><span class="line">    <span class="comment">//In this case it's only one list variable, which will now be referenced by listObj</span></span><br><span class="line">    <span class="keyword">if</span> (! PyArg_ParseTuple( args, <span class="string">"O"</span>, &amp;listObj ))</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//length of the list</span></span><br><span class="line">    <span class="keyword">long</span> length = PyList_Size(listObj);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//iterate over all the elements</span></span><br><span class="line">    <span class="keyword">int</span> i, sum =<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; length; i++) &#123;</span><br><span class="line">        <span class="comment">//get an element out of the list - the element is also a python objects</span></span><br><span class="line">        PyObject* temp = PyList_GetItem(listObj, i);</span><br><span class="line">        <span class="comment">//we know that object represents an integer - so convert it into C long</span></span><br><span class="line">        <span class="keyword">long</span> elem = PyInt_AsLong(temp);</span><br><span class="line">        sum += elem;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//value returned back to python code - another python object</span></span><br><span class="line">    <span class="comment">//build value here converts the C long to a python integer</span></span><br><span class="line">    <span class="keyword">return</span> Py_BuildValue(<span class="string">"i"</span>, sum);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">//This is the docstring that corresponds to our 'add' function.</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">char</span> addList_docs[] =</span><br><span class="line"><span class="string">"add(  ): add all elements of the list\n"</span>;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/* This table contains the relavent info mapping -</span></span><br><span class="line"><span class="comment">   &lt;function-name in python module&gt;, &lt;actual-function&gt;,</span></span><br><span class="line"><span class="comment">   &lt;type-of-args the function expects&gt;, &lt;docstring associated with the function&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">static</span> PyMethodDef addList_funcs[] = &#123;</span><br><span class="line">    &#123;<span class="string">"add"</span>, (PyCFunction)addList_add, METH_VARARGS, addList_docs&#125;,</span><br><span class="line">    &#123;<span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>, <span class="literal">NULL</span>&#125;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">   addList is the module name, and this is the initialization block of the module.</span></span><br><span class="line"><span class="comment">   &lt;desired module name&gt;, &lt;the-info-table&gt;, &lt;module's-docstring&gt;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">PyMODINIT_FUNC <span class="title">initaddList</span><span class="params">(<span class="keyword">void</span>)</span></span>&#123;</span><br><span class="line">    Py_InitModule3(<span class="string">"addList"</span>, addList_funcs,</span><br><span class="line">            <span class="string">"Add all ze lists"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li><code>Python.h</code>头文件中包含了所有需要的类型(Python对象类型的表示)和函数定义(对Python对象的操作)；</li><li><p>接下来我们编写将要在Python调用的函数, 函数传统的命名方式由<code>{模块名}_{函数名}</code>组成，所以我们将其命名为<code>addList_add</code>；</p></li><li><p>然后填写想在模块内实现函数的相关信息表，每行一个函数，以空行作为结束 </p></li><li>最后的模块初始化块签名为<code>PyMODINIT_FUNC init{模块名}</code>。</li></ul><p>函数<code>addList_add()</code>接受的参数类型为PyObject类型结构(同时也表示为元组类型，因为Python中万物皆为对象，所以我们先用PyObject来定义)。传入的参数则通过<code>PyArg_ParseTuple()</code>来解析。第一个参数是被解析的参数变量。第二个参数是一个字符串，告诉我们如何去解析元组中每一个元素。字符串的第n个字母正是代表着元组中第n个参数的类型。例如，”i”代表整形，”s”代表字符串类型, “O”则代表一个Python对象。接下来的参数都是你想要通过<code>PyArg_ParseTuple()</code>函数解析并保存的元素。这样参数的数量和模块中函数期待得到的参数数量就可以保持一致，并保证了位置的完整性。例如，我们想传入一个字符串，一个整数和一个Python列表，可以这样去写:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">char</span> *s;</span><br><span class="line">PyObject* <span class="built_in">list</span>;</span><br><span class="line">PyArg_ParseTuple(args, <span class="string">"isO"</span>, &amp;n, &amp;s, &amp;<span class="built_in">list</span>);</span><br></pre></td></tr></table></figure><p>在这种情况下，我们只需要提取一个列表对象，并将它存储在listObj变量中。然后用列表对象中的<code>PyList_Size()</code>函数来获取它的长度。就像Python中调用<code>len(list)</code>。现在我们通过循环列表，使用<code>PyList_GetItem(list, index)</code>函数来获取每个元素。这将返回一个<code>PyObject*</code>对象。既然Python对象也能表示PyIntType，我们只要使用<code>PyInt_AsLong(PyObj *)</code>函数便可获得我们所需要的值。我们对每个元素都这样处理，最后再得到它们的总和。总和将被转化为一个Python对象并通过<code>Py_BuildValue()</code>返回给Python代码，这里的i表示我们要返回一个Python整形对象。至此，我们已经编写完C模块了，将下列代码保存为<code>setup.py</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># build the modules</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup, Extension</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">'addList'</span>, version=<span class="string">'1.0'</span>, ext_modules=[Extension(<span class="string">'addList'</span>, [<span class="string">'adder.c'</span>])])</span><br></pre></td></tr></table></figure><p>并且运行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py install</span><br></pre></td></tr></table></figure><p><strong>我用的是python3.6，在执行install时出现了错误，然后是python2.7执行成功</strong>。如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python2 setup.py install</span><br></pre></td></tr></table></figure><p>现在应该已经将我们的C文件编译安装到我们的Python模块中了。接下来，让我们来验证下我们的模块是否有效：</p><p>在一番辛苦后，让我们来验证下我们的模块是否有效</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># module that talks to the C code</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> addList</span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Sum of List - "</span> + str(l) + <span class="string">" = "</span> +  str(addList.add(l)))</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sum of List - [1, 2, 3, 4, 5] = 15</span><br></pre></td></tr></table></figure><p>如你所见，我们已经使用<code>Python.h</code>（API）成功开发出了我们第一个Python C扩展。这种方法看似复杂，但你一旦习惯，它将变的非常有效。Python调用C代码的另一种方式便是使用Cython让Python编译的更快。但是Cython和传统的Python比起来可以将它理解为另一种语言，所以我们就不在这里过多描述了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;首先，我们要明确为什么要在Python中调用C？比较常见原因如下： &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;你要提升代码的运行速度，而且你知道C要比Python快50倍以上。&lt;/li&gt;
&lt;li&gt;C语言中有很多传统类库，而且有些正是你想要的，但你又不想用Python去重写它们&lt;/li&gt;
&lt;li&gt;想对从内存到文件接口这样的底层资源进行访问。 &lt;/li&gt;
&lt;li&gt;不需要理由，Just想玩玩。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更详细请点击&lt;a href=&quot;https://blog.csdn.net/qq_35636311/article/details/78255568&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>推荐系统论文笔记</title>
    <link href="http://yoursite.com/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</id>
    <published>2019-05-21T08:58:20.000Z</published>
    <updated>2019-11-28T13:36:23.699Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍一些自己阅读推荐统的论文笔记，会不定期更新，目前是排序学习的一些相关论文。</p><ul><li>排序是对一组物品列表按照某种方式进行排序，来最大化整个列表的效用的过程，广泛应用于搜索引擎、推荐系统、机器翻译、对话系统甚至计算生物学。一些监督机器学习技术经常被广泛应用在这些问题中，这些技术称作排序学习技术。<a id="more"></a></li></ul><h1 id="《DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction》"><a href="#《DeepFM-A-Factorization-Machine-based-Neural-Network-for-CTR-Prediction》" class="headerlink" title="《DeepFM: A Factorization-Machine based Neural Network for CTR Prediction》"></a>《DeepFM: A Factorization-Machine based Neural Network for CTR Prediction》</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><a href="https://arxiv.org/pdf/1703.04247.pdf" target="_blank" rel="noopener">DeepFM</a>是2017年提出的一种端到端模型，它解决了Wide &amp; Deep Learning中<strong>手工设计特征</strong>的问题。DeepFM融合了Factorization Machine(FM)的推荐优势和Deep Learing的特征提取优势。其中FM部分能够建模特征间的低阶关联，Deep部分能够建模特征间的高阶关联。</p><p>DeepFM的主要优势如下：</p><ul><li>FM+DNN：FM部分实现低阶的特征提取，DNN实现高阶的特征提取。同时无需做特征工程。</li><li>训练高效：DeepFM的FM部分和Deep部分共享同一输入向量和嵌入向量。</li></ul><h2 id="DeepFM方法详解"><a href="#DeepFM方法详解" class="headerlink" title="DeepFM方法详解"></a>DeepFM方法详解</h2><p>假设数据集包含$n$个样本$(\mathbf{x},y)$，其中$y$是标签，取0和1；$\mathbf{x}$是由m个fields组成的数据，每条数据由$(u,i)$数据对组成，$u$和$i$分别指的是点播影院辅助信息和影片描述特征，它们可以包括类别字段（比如点播影院的省份、城市、影片的类型等），又可以包括连续值特征（比如影片评分等），其中类别型特征使用one-hot方式来表示，连续值特征先根据其分布离散化后，再使用one-hot方式表示。</p><h3 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h3><p>DeepFM包含两部分：FM部分与Deep部分，分别负责低阶特征的提取和高阶特征的提取。这两部分共享同样的输入。对于特征$i$，标量$w_i$ 用于表示其一阶权重。隐向量$\mathbf{v}_i$用以表示特征$i$与其他特征之间的相互作用。$\mathbf{v}_i$在FM部分是用以对2阶特征进行建模，即特征之间的相互作用；$\mathbf{v}_i$输入到Deep部分则是用以进行高阶特征建模。DeepFM的预测结果可以写为：</p><script type="math/tex; mode=display">\hat{y}=\sigma(y_{FM}(\mathbf{x})+y_{DNN}(\mathbf{x}))</script><p>DeepFM的框架如下：</p><img src="/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DeepFM%E7%9A%84%E6%A1%86%E6%9E%B6.png" class title="This is an image"><p>上图是，它将Wide&amp;Deep Learning中的Wide组件使用FM替换，并且两部分共享同一输入向量和嵌入向量。</p><p>其中最下面的稀疏特征层中，黄色圈和蓝色圈表示经过one-hot编码后原始稀疏特征(分别取值1和0)，可以看到每个field中的黄色圈都有一条直接指向FM层中的Addition黑色的线，这代表FM中的一阶项；由Dense Embeddings层中指向FM的红色线代表的是FM模型中二阶项的$w_{ij}=⟨\mathbf{v}_i,\mathbf{v}_j⟩$中的$\mathbf{v}_i$和$\mathbf{v}_j$；而指向Deep部分中的黑色的线是稀疏特征嵌入后的稠密特征。</p><p>另外，之所以说不需要人工特征工程，是因为交叉特征工作是模型自动进行的，在输入侧不需要手动进行交叉特征处理。</p><h3 id="FM部分"><a href="#FM部分" class="headerlink" title="FM部分"></a>FM部分</h3><p>FM部分是一个因子分解机。因为引入了隐变量的原因，对于几乎不出现或者很少出现的隐变量，FM也可以很好的学习。</p><img src="/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DeepFM%E4%B8%ADFM%E9%83%A8%E5%88%86.png" class title="This is an image"><p>FM的输出公式如下：</p><script type="math/tex; mode=display">y(\mathbf{x})=w_0+\sum_{i=1}^n(w_i\mathbf{x})+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}<\mathbf{v}_i,\mathbf{v}_j>\mathbf{x}_i\mathbf{x}_j</script><h3 id="Deep部分"><a href="#Deep部分" class="headerlink" title="Deep部分"></a>Deep部分</h3><p>深度部分是一个前馈神经网络,用以获取高阶特征间相互作用。与图像或者语音这类连续而且密集的输入不同，它的的输入一般是极其稀疏，超高维，离散型和连续型混合且多字段的。因此这儿需要在第一层隐含层之前，引入一个嵌入层来将输入向量压缩到低维稠密向量。</p><img src="/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DeepFM%E4%B8%ADDeep%E9%83%A8%E5%88%86.png" class title="This is an image"><p><strong>嵌入层的网络结构</strong>如下：</p><img src="/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/DeepFM%E5%B5%8C%E5%85%A5%E9%83%A8%E5%88%86%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" class title="This is an image"><p>这个网络结构有两个关键点：</p><ul><li>虽然输入的每个field向量长度不一样，但是它们embedding出来的长度是固定的，上图示例的嵌入长度是k=5；</li><li>FM中的隐向量$V$作为该嵌入层的权重矩阵，以实现输入的field向量压缩到embedding向量的转换。隐向量$v_{ik}$是嵌入层中第i个field连接到嵌入层第k个节点的权重。</li></ul><p>这里的第二点可以这样理解：假设我们的k=5，首先，对于输入的一条记录，同一个field 只有一个位置是1，那么在由输入得到dense向量的过程中，输入层只有一个神经元起作用，得到的dense向量其实就是输入层embedding层该神经元相连的五条线的权重，即$v<em>{i1}，v</em>{i2}，v<em>{i3}，v</em>{i4}，v_{i5}$ 。这五个值组合起来就是我们在FM中所提到的$\mathbf{v}_i$,在FM部分和DNN部分，这一块是共享权重的，对同一个特征来说，得到的$\mathbf{v}_i$是相同的。</p><h2 id="与其他神经网络的关系"><a href="#与其他神经网络的关系" class="headerlink" title="与其他神经网络的关系"></a>与其他神经网络的关系</h2><p>下面是FNN、PNN和Wide&amp;Deep模型框架：</p><img src="/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/FNN%E3%80%81PNN%E5%92%8CWide&Deep%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6.png" class title="This is an image"><p><strong>FNN</strong>是一个FM初始化的前馈神经网络，与DeepFM不同，它是用FM预训练出来的$V$来对Deep部分进行初始化，仅提取到了高阶特征，它有两个限制。</p><ul><li>嵌入层参数可能受FM影响过大。</li><li>预训练阶段引入的开销降低了效率。</li></ul><p><strong>PNN</strong>为了捕获高阶交叉特征，PNN在嵌入层和第一层隐藏层之间增加了一个product层，根据 product 的不同，衍生出三种 PNN：IPNN，OPNN，PNN* 分别对应内积、外积、两者混合。和FNN一样，它只能学习到高阶的特征组合，没有对于1阶和2阶特征进行建模。</p><p><strong>Wide&amp;Deep</strong>将Wide组件和Deep组件进行融合，同时学习低阶和高阶特征，但是wide部分需要人工构造交叉特征。</p><p>下面是它们关于是否需要预训练、是否提取了高阶、低阶特征以及是否需要特征工程的比较：</p><h2 id="实验部分"><a href="#实验部分" class="headerlink" title="实验部分"></a>实验部分</h2><p>在这篇论文的实验部分中，介绍了所使用的数据集Criteo Dataset和Company∗ Dataset、评估指标AUC和Logloss、性能评估(CPU训练时间和GPU训练时间)、以及超参数学习（包括激活函数、弃权（Dropout）概率、每层神经元数目、隐藏层数目和网络形状）。</p><h1 id="《BPR-Bayesian-Personalized-Ranking-from-Implicit-Feedback》"><a href="#《BPR-Bayesian-Personalized-Ranking-from-Implicit-Feedback》" class="headerlink" title="《BPR: Bayesian Personalized Ranking from Implicit Feedback》"></a>《BPR: Bayesian Personalized Ranking from Implicit Feedback》</h1><h2 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h2><p>目前排序算法大致被分为三类，分别是点对排序(Pointwise)、成对排序(Pairwise)和列表排序(Listwise)。BPR(Bayesian Personalized Ranking，贝叶斯个性化排序)就是属于成对方法（Pairwise）中的一种，成对排序对物品顺序关系是否合理进行判断，判断任意两个物品组成的物品对$<item1,item2>$，是否满足顺序关系，从而最终完成物品的排序任务来得到推荐列表。</item1,item2></p><h2 id="BPR建模思路"><a href="#BPR建模思路" class="headerlink" title="BPR建模思路"></a>BPR建模思路</h2><p>它基于这样的假设，比起其他没有被交互过的物品而言，用户更喜爱对交互过物品(而对于用户交互过的物品对之间不假设偏序关系，同样，对于用户没有交互过的物品对之间也不假设偏序关系)。从而将 “用户-物品 ”交互矩阵可以转换为物品对偏序关系矩阵。如下图所示：</p><img src="/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/BPR%E8%BD%AC%E6%8D%A2%E7%9F%A9%E9%98%B5.png" class title="This is an image"><p>上述是交互矩阵转换为物品偏序对矩阵的过程。所有用户的物品偏序对矩阵可以表示成3元组$&lt; u,i,j >$，该三元组的含义为：相对于物品$j$，用更喜欢物品$i$，使用符号$i &gt;<em>u j$表示。令$D_s = {(u,i,j)|i \in I_u^+  \cap j \in I \backslash I_u^+ }$。$I</em>{u}^{+}$表示用户$u$交互过的物品集合，同理，$U_{i}^{+}$表示与物品$i$交互过的用户集合，</p><p>在此基础上 ,作者提出了基于贝叶斯的个性化排序算法，其目标是最大化物品排序的后验概率。它有如下两个假设：一是每个用户对物品的偏好与其他用户无关，即，用户的偏好行为相互独立；二是每个用户在物品$i$和物品$j$之间的偏好和其他商品无关，即，每个用户对不同物品的偏序相互独立。</p><p>在BPR中，排序关系符号$&gt;_u$满足完整性、反对称性和传递性，即对于用户集$U$和物品集$I$：</p><ol><li>完整性：$\forall i,j\in I:i\neq j \Rightarrow i &gt;_u j \cup j&gt;_ui$</li><li>反对称性：$\forall i,j\in I:i&gt;_uj \cap j&gt;_ui\Rightarrow i=j$</li><li>传递性：$\forall i,j,k\in I:i&gt;_uj \cap j&gt;_uk\Rightarrow i&gt;_uk$</li></ol><p>另外，BPR用到了和funkSVD相似的矩阵分解模型，它满足：</p><script type="math/tex; mode=display">\overline{X}=WH^T</script><p>其中左边$\overline{X}$表示BPR对于用户$U$和物品集$I$的对应的$U\times I$的预测排序矩阵，右边$H^T$分别表示希望得到的分解后的用户矩阵$W(|U|\times k)$和物品矩阵$H(|I|\times k)$。BPR是基于用户维度的，故，对于任意一个用户$u$，对应的任一个物品$i$，我们期望有：</p><script type="math/tex; mode=display">\overline{x}_{ui}=w_u \cdot h_i=\sum_{f=1}^{k}w_{uf}h_{if}</script><p>最终目标是希望找到合适的矩阵$W$和$H$，让$\overline{X}$和$X$最相似。下面第3部分BPR的优化思路部分会介绍它和funkSVD有何不同。</p><h2 id="BPR的算法优化思路"><a href="#BPR的算法优化思路" class="headerlink" title="BPR的算法优化思路"></a>BPR的算法优化思路</h2><p>BPR基于最大后验估计$P(W,H|&gt;_u)$来求解模型参数$W,H$，这里我们用$\Theta$来表示模型的参数$W,H$，$&gt;_u$代表用户$u$对应所有商品的全序关系，则优化目标是$P(\Theta|&gt;_u)$。根据贝叶斯公式，我们有：</p><script type="math/tex; mode=display">P(\Theta|>_u)=\frac{P(>_u|\Theta)P(\Theta)}{P(>_u)}</script><p>由于我们求解假设了用户的排序和其他用户无关，那么对任意用户$u$来说，$P(&gt;_u)$对所有的物品一样，所以有：</p><script type="math/tex; mode=display">P(\Theta|>_u)\propto P(>_u|\Theta)P(\Theta)</script><p>可以看出优化目标转化为两部分。$P(&gt;_u|\Theta)$和样本数据集$D_s$有关，$P(\Theta)$和样本数据集$D_s$无关。</p><p>对于第一部分（似然部分），我们假设了用户之间偏好独立，对不同商品偏序独立，故有：</p><script type="math/tex; mode=display">\prod_{u \in U}P(>_u|\Theta) = \prod_{(u,i,j)\in (U\times I \times I)}P(i>_uj|\Theta)^{\delta((u,i,j)\in D_s)}(1-P(i>_uj|\Theta))^{\delta((u,j,i)\notin D_s)}</script><p>其中，</p><script type="math/tex; mode=display">\delta(b)=\begin{cases}1,& if \ b \ is \ true\\0,& else\end{cases}</script><p>根据第2部分介绍到的完整性和反对称性，优化目标的第一部分可以简化为：</p><script type="math/tex; mode=display">\prod_{u \in U}P(>_u|\Theta) = \prod_{(u,i,j)\in D_s}P(i>_uj|\Theta)</script><p>而对于$P(i&gt;_uj|\Theta)$这个概率，我们可以使用下面这个式子来代替：</p><script type="math/tex; mode=display">P(i>_uj|\Theta)=\sigma(\overline{x}_{uij}(\Theta))</script><p>其中$\sigma(x)$是sigmoid函数，对于$\overline{x}<em>{uij}(\Theta)$，我们要满足当$i&gt;_uj$时，$\overline{x}</em>{uij}(\Theta)&gt;0$，反之，当$j &gt;<em>u i$时，$\overline{x}</em>{uij}(\Theta)&lt;0$；那么最简单表示这个性质的方法就是：</p><script type="math/tex; mode=display">\overline{x}_{uij}(\Theta) = \overline{x}_{ui}(\Theta)-\overline{x}_{uj}(\Theta)</script><p>而$\overline{x}<em>{ui},\overline{x}</em>{uj}$就是矩阵$\overline{X}$对应位置的值，为了方便，暂不写$\Theta$；最终，第一部分的优化目标转化为：</p><script type="math/tex; mode=display">\prod_{u \in U}P(>_u|\Theta) = \prod_{(u,i,j)\in D_s}\sigma(\overline{x}_{ui}-\overline{x}_{uj})</script><p>对于第二部$P(\Theta)$，即，先验部分，可以根据参数的假设分布选择，如高斯分布：均值为0，协方差矩阵是$\lambda_\Theta I$，如下：</p><script type="math/tex; mode=display">P(\Theta) \sim N(0,\lambda_\Theta I)</script><p>其中$\lambda_\Theta $是模型的正则化参数。最终，可以得到最大对数后验估计函数：</p><script type="math/tex; mode=display">\begin{aligned}\ln P(\Theta|>_u)&\propto \ln P(>_u|\Theta)P(\Theta)\\&=\ln\prod_{(u,i,j)\in D_s}\sigma(\overline{x}_{ui}-\overline{x}_{uj})+\ln P(\Theta)\\&=\sum_{(u,i,j)\in D}\ln \sigma(\overline{x}_{ui}-\overline{x}_{uj})+\lambda||\Theta||^2\end{aligned}</script><p>由于</p><script type="math/tex; mode=display">\overline{x}_{ui}-\overline{x}_{uj} = \sum_{f=1}^{k}w_{uf}h_{if}-\sum_{f=1}^{k}w_{uf}h_{jf}</script><p>我们求偏导可以得到如下式子：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial (\overline{x}_{ui} - \overline{x}_{uj})}{\partial \Theta} &= \begin{cases} (h_{if}-h_{jf})& {if\; \Theta = w_{uf}}\\ w_{uf}& {if\;\Theta = h_{if}} \\ -w_{uf}& {if\;\Theta = h_{jf}}\end{cases}\end{aligned}</script><h2 id="BPR的算法流程"><a href="#BPR的算法流程" class="headerlink" title="BPR的算法流程"></a>BPR的算法流程</h2><p>下面简要总结下BPR的算法训练流程：</p><p>输入：训练集$D_s$三元组，梯度步长$\alpha$， 正则化参数$\lambda$,分解矩阵维度$k$。　　　　　　　　　　</p><p>输出：模型参数，矩阵W,H。</p><ol><li><p>随机初始化矩阵$W,H$</p></li><li><p>迭代更新参数：</p><script type="math/tex; mode=display">\begin{aligned}w_{uf}=w_{uf}+\alpha(\sum_{(u,i,j)\in D_s}\frac{1}{1+e^{\overline{x}_{ui}-\overline{x}_{uj}}}(h_{if}-h_{jf})+\lambda w_{uf})\\h_{if}=h_{if}+\alpha(\sum_{(u,i,j)\in D_s}\frac{1}{1+e^{\overline{x}_{ui}-\overline{x}_{uj}}}w_{uf}+\lambda h_{if})\\h_{jf}=h_{jf}+\alpha(\sum_{(u,i,j)\in D_s}\frac{1}{1+e^{\overline{x}_{ui}-\overline{x}_{uj}}}w_{uf}+\lambda h_{jf})\end{aligned}</script></li><li><p>如果$W,H$收敛，则算法结束，输出$W,H$；否则回到步骤2。</p></li></ol><p>当我们得到$W,H$后，可以计算出每一个用户$u$对应的任意一个商品的排序分数，最终选择排序分最高的若干商品输出。</p><h2 id="BPR总结"><a href="#BPR总结" class="headerlink" title="BPR总结"></a>BPR总结</h2><p>BPR是基于矩阵分解的一种排序算法，但是和funkSVD之类的算法比，它不是做全局的评分优化，而是针对每一个用户自己的商品喜好分别做排序优化。这篇文章的主要贡献就是提出了上述BPR优化目标。BPR优化目标中的模型$\Theta$可以使用多种多样的模型，包括协同过滤、神经网络等等。</p><h1 id="《From-RankNet-to-LambdaRank-to-LambdaMART-An-Overview》"><a href="#《From-RankNet-to-LambdaRank-to-LambdaMART-An-Overview》" class="headerlink" title="《From RankNet to LambdaRank to LambdaMART: An Overview》"></a>《From RankNet to LambdaRank to LambdaMART: An Overview》</h1><h2 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h2><p>LambdaMART是LambdaRank的提升树版本，而LambdaRank又是基于pairwise的RankNet。因此LambdaMART本质上也是属于pairwise排序算法，只不过引入Lambda梯度后，还显示的考察了列表级的排序指标，如NDCG等，将排序问题转化为回归决策树问题。因此，它算作是listwise中的一种排序算法。</p><p>LambdaMART模型从名字上可以拆分成Lambda和MART两部分：</p><ul><li>MART表示底层训练模型用的是MART（Multiple Additive Regression Tree），也就是GBDT（GradientBoosting Decision Tree）。</li><li>Lambda是<strong>MART求解过程使用的梯度</strong>，其物理含义是一个待排序的物品列表下一次迭代时应该调整的排序方向（向上或者向下）和强度。</li></ul><p>将MART和Lambda组合起来就是我们要介绍的LambdaMART。</p><p>下面逐个介绍RankNet、LambdaRank和LambdaMART三个模型。</p><h2 id="RankNet"><a href="#RankNet" class="headerlink" title="RankNet"></a>RankNet</h2><p>RankNet是一个pairwise模型，它和BPR非常像。创新之处都在于，原本Ranking常见的排序问题评价指标（NDCG、ERR、MAP和MRR）都无法求梯度，因此没法直接对评价指标做梯度下降，而它们将不适宜用梯度下降求解的 Ranking 问题，转化为对偏序概率的交叉熵损失函数的优化问题，从而适用梯度下降方法。</p><p>RankNet的最终目标是得到一个带参的算分函数：</p><script type="math/tex; mode=display">s=f(x;w)</script><p>根据这个算分函数，我们可以计算物品$x_i$和$x_j$的得分$s_i$和$s_j$，即：</p><script type="math/tex; mode=display">s_i = f(x_i;w)   \ \ \ \ s_j=f(x_j;w)</script><p>然后根据得分计算二者的偏序概率：</p><script type="math/tex; mode=display">P_{ij}=P(x_i\rhd x_j)=\frac{1}{1+e^{-\sigma\cdot(s_i-s_j)}}</script><p>其中$\sigma$是Sigmoid函数的参数，决定了Sigmoid曲线的形状。RankNet证明了如果知道一个待排序物品的排列中相邻两个物品之间的排序概率，则通过推导可以算出每两个物品之间的排序概率。因此对于一个待排序物品序列，只需计算相邻物品之间的排序概率，不需要计算所有pair，减少计算量。</p><p>接下来定义标签$S<em>{ij}={1,-1,0}$，$\overline{P}</em>{ij}=\frac{1}{2}(S<em>{ij}+1)$是物品$x_i$比$x_j$排序靠前的真实概率，即当$x_i\rhd x_j$时，$S</em>{ij}=1$，$\overline{P}<em>{ij}=$1；当$x_j\rhd x_i$时，$S</em>{ij}=-1$，$\overline{P}<em>{ij}=0$；否则$S</em>{ij}=0$，$\overline{P}_{ij}=\frac{1}{2}$。</p><p>定义交叉熵作为损失函数衡量$P<em>{ij}$和$\overline{P}</em>{ij}$的拟合程度：</p><script type="math/tex; mode=display">\begin{aligned}C_{ij}&=-\overline{P}_{ij}\log{P_{ij}}-(1-\overline{P}_{ij})\log(1-P_{ij})\\&=-\frac{1}{2}(1+S_{ij})\log{P_{ij}}-(1-\frac{1}{2}(1+S_{ij}))\log(1-P_{ij})\\&=-\frac{1}{2}(1+S_{ij})\log{P_{ij}}-\frac{1}{2}(1-S_{ij})\log(1-P_{ij})\\&=-\frac{1}{2}S_{ij}\log{\frac{1-P_{ij}}{P_{ij}}}-\frac{1}{2}\log(1-P_{ij})\log{P_{ij}}\\&= -\frac{1}{2}S_{ij}(-\sigma\cdot(s_i-s_j))-\frac{\partial{P_{ij}}}{\partial{\sigma\cdot(s_i-s_j)}}\\&= -\frac{1}{2}S_{ij}(-\sigma\cdot(s_i-s_j))-\frac{1}{2}(-\sigma\cdot(s_i-s_j)-2\log(1+e^{-\sigma\cdot(s_i-s_j)}))\\&=\frac{1}{2}(1-S_{ij})\sigma\cdot(s_i-s_j)+\log(1+e^{-\sigma\cdot(s_i-s_j)})\end{aligned}</script><p>上式利用了性质，$P=\frac{1}{1+e^{-x}}\rightarrow x=\log(\frac{P}{1-P}),P(1-P)=\frac{\partial P}{\partial x}$。</p><p>该损失函数有以下两个特点：</p><ul><li>当两个相关性不同的物品算出来的模型分数相同时，因为损失函数后半部分为$\log(1+e^{-\sigma\cdot(s_i-s_j)})$，损失函数的值大于0，仍会对这对pair做惩罚，使他们的排序位置区分开。</li><li>损失函数是一个类线性函数，可以有效减少异常样本数据对模型的影响，因此具有鲁棒性。</li></ul><p>RankNet采用神经网络模型优化损失函数，采用梯度下降法求解：</p><script type="math/tex; mode=display">w_k=w_k-\eta\frac{\partial C}{\partial w_k}</script><h2 id="LambdaRank"><a href="#LambdaRank" class="headerlink" title="LambdaRank"></a>LambdaRank</h2><p>在介绍LambdaRank的动机之前，我们先从一张图来考察RankNet学习过程中，列表的排序变化。</p><img src="/2019/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/05/21/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/RankNet%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B.png" class title="This is an image"><p>如上所示，每个线条表示物品，蓝色表示相关物品，灰色表示不相关物品，RankNet以pairwise error的方式计算损失，在某次迭代中，RankNet 将物品的顺序从左边变成了右边。于是我们可以看到：</p><ul><li>RankNet 的梯度下降表现在结果的整体变化中是逆序对的下降。左图中，2~14不相关物品都排在了15号相关物品之前，这些不相关物品和15号物品构成的逆序对共13个，因此损失等价于为13；而右图中，将1号相关物品降到了4号，15号相关物品上升到了10号，此时逆序对的数量为3+8=11个，因此损失等价于11.</li><li>对于一些强调最靠前的TopK个物品的排序指标(NDCG、ERR等)而言，上述优化不是理想的。例如，右图下一次迭代，在Ranknet中梯度优化方向如黑色箭头所示，此时损失可以下降到8；然而对于NDCG指标而言，我们更愿意看到红色箭头所示的优化方向（此时Ranknet同样是8，但是NDCG指标相比前一种情况上升了），即关注靠前位置的相关物品排序位置的提升。</li></ul><p>LambdaRank正是基于这个思想演化而来，其中Lambda指的就是红色箭头，代表下一次迭代优化的方向和强度，也就是梯度。故，LambdaRank先对$\frac{\partial C}{\partial w_k}$做因式分解，如下：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial w_k}=\sum_{(i,j)\in P}\frac{\partial C_{ij}}{\partial w_k}=\sum_{(i,j)\in P}(\frac{\partial C_{ij}}{\partial s_i}\frac{\partial s_i}{\partial w_k}+\frac{\partial C_{ij}}{\partial s_j}\frac{\partial s_j}{\partial w_k})</script><p>其中：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial C_{ij}}{\partial s_i}&=\frac{\partial \frac{1}{2}(1-S_{ij})\sigma\cdot(s_i-s_j)+\log(1+e^{-\sigma\cdot(s_i-s_j)})}{\partial s_i}\\&=\sigma\cdot(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma\cdot(s_i-s_j)}})\\&=-\frac{\partial C_{ij}}{\partial s_j}\end{aligned}</script><p>代入上式得：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial w_k}=\sum_{(i,j)\in P}(\sigma\cdot(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma\cdot(s_i-s_j)}}))(\frac{\partial s_i}{\partial w_k}-\frac{\partial s_j}{\partial w_k})</script><p>令：</p><script type="math/tex; mode=display">\lambda_{ij}=\frac{\partial C_{ij}}{\partial s_i}=\sigma\cdot(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma\cdot(s_i-s_j)}})</script><p>考虑对于有序物品对$(i,j)$，有$S_{ij}=1$，于是有简化：</p><script type="math/tex; mode=display">\lambda_{ij}=-\frac{\sigma}{1+e^{\sigma\cdot(s_i-s_j)}}</script><p>因此，考虑偏序对的对称性，对每个物品$x_i$，其Lambda为：</p><script type="math/tex; mode=display">\lambda_i=\sum_{(i,j)\in P}\lambda_{ij}-\sum_{(j,i)\in P}\lambda_{ij}</script><p>每个物品移动的方向和趋势取决于其他所有与之label不同的物品（比它靠前或比它靠后都考虑）。$i$比越多的物品$j$的真实排名越优，则$\lambda_i$越大；反之，越小。</p><p>同时LambdaRank在此基础上，考虑排序评价指标$Z$（比如$NDCG,\Delta|NDCG|$），把交换两个物品的位置引起的评价指标的变化$|\Delta Z_{ij}|$作为其中一个因子，如下：</p><script type="math/tex; mode=display">\lambda_{ij}=-\frac{1}{1+e^{\sigma\cdot(s_i-s_j)}}|\Delta Z_{ij}|</script><p>可以看出，LambdaRank不是直接显示定义损失函数再求梯度的方式对排序问题进行求解，而是分析排序问题需要的梯度的物理意义，直接定义梯度，我们可以反推出LambdaRank的损失函数$L<em>{ij}=\log(1+e^{-\sigma\cdot(s_i-s_j)})|\Delta Z</em>{ij}|$。</p><p>LambdaRank相比RankNet的优势在于分解因式后训练速度变快，同时考虑了评价指标，直接对问题求解，效果更明显。</p><h2 id="LambdaMART"><a href="#LambdaMART" class="headerlink" title="LambdaMART"></a>LambdaMART</h2><p>LambdaRank重新定义了梯度，赋予了梯度新的物理意义，因此，所有可以使用梯度下降法求解的模型都可以使用这个梯度，MART（即GBDT）就是其中一种，MART的原理是直接在函数空间对函数进行求解，模型结果由许多棵树组成，每棵树的拟合目标是损失函数的梯度，在LambdaMART中就是Lambda。就变成这样：</p><ul><li>MART是一个框架，缺少一个<strong>梯度</strong>；</li><li><p>LambdaRank定义了一个<strong>梯度</strong>。</p><p>下面介绍LambdaMART的每一步工作：</p></li></ul><ol><li>每棵数的训练会先遍历所有的训练数据（label不同的物品对pair），计算每个pari互换位置导致的指标变化$|\Delta Z<em>{ij}|$以及Lambda，即$\lambda</em>{ij}=-\frac{1}{1+e^{\sigma\cdot(s<em>i-s_j)}}|\Delta Z</em>{ij}|$，然后计算每个物品档的Lambda：$\lambda<em>i=\sum</em>{(i,j)\in P}\lambda<em>{ij}-\sum</em>{(j,i)\in P}\lambda_{ij}$，再计算每个$\lambda_i$的导数$w_i$，用于后面的牛顿迭代法(Newton step)求解叶子节点的数值。</li><li>创建回归树去拟合第一步生成的$\lambda<em>i$，划分树节点的标准是均方差（MSE），生成一棵叶子节点数为k的回归树$R</em>{km}$，$m$表示第$m$棵树。</li><li>对第二步生成的回归树，计算每个叶子节点的数值，采用牛顿迭代法求解，即对落入该叶子节点的物品集，用公式$\frac{\sum<em>{x_i\in R</em>{km}}\lambda<em>i}{\sum</em>{x<em>i\in R</em>{km}}w_i}$计算该叶子节点的输出值。</li><li>更新模型，将当前学习到的回归树加入到已有的模型中做回归。</li></ol><p>LambdaMART有很多优势：</p><ol><li>适用于排序场景：不是传统的通过分类或者回归的方法求解排序问题，而是直接求解。</li><li>损失函数可导：通过损失函数的转换，将类似于NDCG这种无法求导的排序评价指标转换成可以求导的函数，并且赋予了梯度的实际物理意义。</li><li>增量学习：由于每次训练可以在已有的模型上继续训练，因此适合于增量学习。</li><li>特征选择：因为是基于MART模型，因此也具有MART的优势，可以学到每个特征的重要性，可以做特征选择。</li><li>组合特征：因为采用树模型，因此可以学到不同特征组合情况。</li><li>适用于正负样本比例失衡的数据：因为模型的训练对象具有不同label的物品对pair，而不是预测每个物品的label，因此对正负样本比例失衡不敏感。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍一些自己阅读推荐统的论文笔记，会不定期更新，目前是排序学习的一些相关论文。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;排序是对一组物品列表按照某种方式进行排序，来最大化整个列表的效用的过程，广泛应用于搜索引擎、推荐系统、机器翻译、对话系统甚至计算生物学。一些监督机器学习技术经常被广泛应用在这些问题中，这些技术称作排序学习技术。&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="推荐系统" scheme="http://yoursite.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐系统" scheme="http://yoursite.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="排序学习" scheme="http://yoursite.com/tags/%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
